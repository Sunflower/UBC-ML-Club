{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Let's load in the data from our training file \n",
    "train_data = [line for line in csv.reader(open('train_data.csv'))]\n",
    "\n",
    "# Get the sentences\n",
    "train_sentences = [sentence for sentence,label in train_data]\n",
    "\n",
    "# Get the labels\n",
    "labels = np.array([int(label) for sentence,label in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test data\n",
    "test_data = [line for line in csv.reader(open('test_data.csv'))]\n",
    "\n",
    "# Get the test sentences\n",
    "test_sentences = [line[0] for line in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " '==edit',\n",
       " 'warring==',\n",
       " 'you',\n",
       " 'appear',\n",
       " 'to',\n",
       " 'be',\n",
       " 'engaged',\n",
       " 'in',\n",
       " 'an',\n",
       " 'edit',\n",
       " 'war',\n",
       " 'with',\n",
       " 'one',\n",
       " 'or',\n",
       " 'more',\n",
       " 'editors',\n",
       " 'according',\n",
       " 'to',\n",
       " 'your',\n",
       " 'reverts',\n",
       " 'at',\n",
       " 'constitution',\n",
       " 'of',\n",
       " 'massachusetts',\n",
       " '.',\n",
       " 'although',\n",
       " 'repeatedly',\n",
       " 'reverting',\n",
       " 'or',\n",
       " 'undoing',\n",
       " 'another',\n",
       " 'editor',\n",
       " \"'s\",\n",
       " 'contributions',\n",
       " 'may',\n",
       " 'seem',\n",
       " 'necessary',\n",
       " 'to',\n",
       " 'protect',\n",
       " 'your',\n",
       " 'preferred',\n",
       " 'version',\n",
       " 'of',\n",
       " 'a',\n",
       " 'page',\n",
       " ',',\n",
       " 'on',\n",
       " 'wikipedia',\n",
       " 'this',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'obstructing',\n",
       " 'the',\n",
       " 'normal',\n",
       " 'editing',\n",
       " 'process',\n",
       " ',',\n",
       " 'and',\n",
       " 'often',\n",
       " 'creates',\n",
       " 'animosity',\n",
       " 'between',\n",
       " 'editors',\n",
       " '.',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'edit',\n",
       " 'warring',\n",
       " ',',\n",
       " 'please',\n",
       " 'discuss',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'with',\n",
       " 'the',\n",
       " 'editor',\n",
       " '(',\n",
       " 's',\n",
       " ')',\n",
       " 'involved',\n",
       " 'and',\n",
       " 'try',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'a',\n",
       " 'consensus',\n",
       " 'on',\n",
       " 'the',\n",
       " 'talk',\n",
       " 'page',\n",
       " '.',\n",
       " '—',\n",
       " 'preceding',\n",
       " 'unsigned',\n",
       " 'comment',\n",
       " 'added',\n",
       " 'by',\n",
       " '2601:283:0',\n",
       " ':',\n",
       " 'f00:410c',\n",
       " ':',\n",
       " 'e87:42ea:3be4',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'warning/threat',\n",
       " \"''\",\n",
       " \"''\",\n",
       " 'reply',\n",
       " 'why',\n",
       " 'are',\n",
       " 'you',\n",
       " 'attacking',\n",
       " 'me',\n",
       " 'at',\n",
       " \"''\"]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# Let's define a method to split a sentence into words\n",
    "word_tokenize(train_sentences[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's tokenize all of the words in the training_data\n",
    "tokenized = []\n",
    "for i,sent in enumerate(train_sentences):\n",
    "    tokenized.append(word_tokenize(sent.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', '==edit', 'warring==', 'you', 'appear', 'to', 'be', 'engaged', 'in', 'an', 'edit', 'war', 'with', 'one', 'or', 'more', 'editors', 'according', 'to', 'your', 'reverts', 'at', 'constitution', 'of', 'massachusetts', '.', 'although', 'repeatedly', 'reverting', 'or', 'undoing', 'another', 'editor', \"'s\", 'contributions', 'may', 'seem', 'necessary', 'to', 'protect', 'your', 'preferred', 'version', 'of', 'a', 'page', ',', 'on', 'wikipedia', 'this', 'is', 'usually', 'seen', 'as', 'obstructing', 'the', 'normal', 'editing', 'process', ',', 'and', 'often', 'creates', 'animosity', 'between', 'editors', '.', 'instead', 'of', 'edit', 'warring', ',', 'please', 'discuss', 'the', 'situation', 'with', 'the', 'editor', '(', 's', ')', 'involved', 'and', 'try', 'to', 'reach', 'a', 'consensus', 'on', 'the', 'talk', 'page', '.', '—', 'preceding', 'unsigned', 'comment', 'added', 'by']\n"
     ]
    }
   ],
   "source": [
    "# Let's flatten our list of (list of words) into a list of words\n",
    "all_words = [word for sent in tokenized for word in sent]\n",
    "print(all_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', ',', 'to', 'i', \"''\", 'and', 'of', 'you', 'a', '!', 'is', 'that', '``', 'it', 'in', 'for', 'this', 'not', 'on', ')', 'be', '(', 'as', ':', 'have', 'are', '?', \"'s\", 'your', 'do', 'article', 'with', 'if', 'was', \"n't\", 'or', 'but', 'an', 'page', 'wikipedia', 'my', 'from', 'me', 'at', 'by', 'can', 'about', 'talk', 'so', 'what', 'there', 'has', 'would', 'please', 'all', 'will', 'they', 'no', 'he', 'just', 'like', 'one', 'should', 'which', '-', 'any', 'been', 'we', 'here', 'more', 'some', '...', 'other', 'who', 'see', 'up', ';', 'edit', 'his', 'also', 'did', 'think', \"'m\", 'how', 'shit', 'know', 'because', 'does', 'only', 'why', 'out', \"'\", 'people', 'when', 'articles', 'am', 'use', 'then', 'now']\n"
     ]
    }
   ],
   "source": [
    "# Identify the 1000 most common words in the corpus and use them as our vocabulary\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(all_words)\n",
    "vocabulary = [word for word,count in counter.most_common(1000)]\n",
    "print(vocabulary[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# We add an \"<UNK>\" token to represent all out-of-vocabulary words\n",
    "vocabulary = [\"<UNK>\"] + vocabulary\n",
    "\n",
    "# Now we invert the array to have a mapping of words to indices\n",
    "word2index = {word:i for i,word in enumerate(vocabulary)}\n",
    "\n",
    "print(vocabulary[10])\n",
    "print(word2index[\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Mind your own business, you troll.\n",
      "\n",
      "Tokenized sentence: ['mind', 'your', 'own', 'business', ',', 'you', 'troll', '.']\n",
      "\n",
      "Vectorized: [ 1.  1.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Let's create a count vectorization of every sentence. \n",
    "# The value at vector[i] will be the number of times vocabulary[i] appears in the sentence.\n",
    "def count_vectorize(sent):\n",
    "    vect = np.zeros(len(vocabulary))\n",
    "    for word in sent:\n",
    "        vect[word2index.get(word, 0)] += 1\n",
    "    \n",
    "    return vect\n",
    "\n",
    "print(\"Original sentence:\", sentences[200])\n",
    "print(\"\")\n",
    "print(\"Tokenized sentence:\", tokenized[200])\n",
    "print(\"\")\n",
    "print(\"Vectorized:\", count_vectorize(tokenized[200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize all of the training data\n",
    "features = np.stack([count_vectorize(sent) for sent in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1001)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Now we have both the features and the labels\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9500, 1001)\n",
      "(500, 1001)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 95%/5%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.05)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Hmmm would I really make make the starter file have the best model...\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.48      0.64       446\n",
      "          1       0.18      0.94      0.30        54\n",
      "\n",
      "avg / total       0.90      0.53      0.60       500\n",
      "\n",
      "0.526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(sum(y_test == y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 1\n",
      " 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0\n",
      " 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0\n",
      " 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1\n",
      " 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0\n",
      " 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0\n",
      " 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
      " 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0\n",
      " 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
      " 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
      " 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
      " 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1\n",
      " 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0\n",
      " 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0\n",
      " 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1\n",
      " 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1\n",
      " 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's train on all of the data \n",
    "clf.fit(features, labels)\n",
    "\n",
    "# Prepare the testing data\n",
    "test_tokenized = [word_tokenize(sent.lower()) for sent in test_sentences]\n",
    "\n",
    "# Count vectorize the sentences\n",
    "test_features = np.stack([count_vectorize(sent) for sent in test_tokenized])\n",
    "\n",
    "y_pred = clf.predict(test_features)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results to a file\n",
    "open(\"predictions.csv\", \"w+\").writelines([str(pred) + \"\\n\" for pred in y_pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
