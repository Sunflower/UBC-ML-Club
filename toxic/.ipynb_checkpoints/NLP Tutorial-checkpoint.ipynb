{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 1942: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-56ef7392811b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Let's load in the data from our training file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Get the sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-56ef7392811b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Let's load in the data from our training file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Get the sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 1942: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Let's load in the data from our training file \n",
    "train_data = [line for line in csv.reader(open('train_data.csv'))]\n",
    "\n",
    "# Get the sentences\n",
    "train_sentences = [sentence for sentence,label in train_data]\n",
    "\n",
    "# Get the labels\n",
    "labels = np.array([int(label) for sentence,label in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the test data\n",
    "test_data = [line for line in csv.reader(open('test_data.csv'))]\n",
    "\n",
    "# Get the test sentences\n",
    "test_sentences = [line[0] for line in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ba86ab04319f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Let's define a method to split a sentence into words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# Let's define a method to split a sentence into words\n",
    "word_tokenize(train_sentences[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's tokenize all of the words in the training_data\n",
    "tokenized = []\n",
    "for i,sent in enumerate(train_sentences):\n",
    "    tokenized.append(word_tokenize(sent.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', '==edit', 'warring==', 'you', 'appear', 'to', 'be', 'engaged', 'in', 'an', 'edit', 'war', 'with', 'one', 'or', 'more', 'editors', 'according', 'to', 'your', 'reverts', 'at', 'constitution', 'of', 'massachusetts', '.', 'although', 'repeatedly', 'reverting', 'or', 'undoing', 'another', 'editor', \"'s\", 'contributions', 'may', 'seem', 'necessary', 'to', 'protect', 'your', 'preferred', 'version', 'of', 'a', 'page', ',', 'on', 'wikipedia', 'this', 'is', 'usually', 'seen', 'as', 'obstructing', 'the', 'normal', 'editing', 'process', ',', 'and', 'often', 'creates', 'animosity', 'between', 'editors', '.', 'instead', 'of', 'edit', 'warring', ',', 'please', 'discuss', 'the', 'situation', 'with', 'the', 'editor', '(', 's', ')', 'involved', 'and', 'try', 'to', 'reach', 'a', 'consensus', 'on', 'the', 'talk', 'page', '.', 'â€”', 'preceding', 'unsigned', 'comment', 'added', 'by']\n"
     ]
    }
   ],
   "source": [
    "# Let's flatten our list of (list of words) into a list of words\n",
    "all_words = [word for sent in tokenized for word in sent]\n",
    "print(all_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', ',', 'to', 'i', \"''\", 'and', 'of', 'you', 'a', '!', 'is', 'that', '``', 'it', 'in', 'for', 'this', 'not', 'on', ')', 'be', '(', 'as', ':', 'have', 'are', '?', \"'s\", 'your', 'do', 'article', 'with', 'if', 'was', \"n't\", 'or', 'but', 'an', 'page', 'wikipedia', 'my', 'from', 'me', 'at', 'by', 'can', 'about', 'talk', 'so', 'what', 'there', 'has', 'would', 'please', 'all', 'will', 'they', 'no', 'he', 'just', 'like', 'one', 'should', 'which', '-', 'any', 'been', 'we', 'here', 'more', 'some', '...', 'other', 'who', 'see', 'up', ';', 'edit', 'his', 'also', 'did', 'think', \"'m\", 'how', 'shit', 'know', 'because', 'does', 'only', 'why', 'out', \"'\", 'people', 'when', 'articles', 'am', 'use', 'then', 'now']\n"
     ]
    }
   ],
   "source": [
    "# Identify the 1000 most common words in the corpus and use them as our vocabulary\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(all_words)\n",
    "vocabulary = [word for word,count in counter.most_common(1000)]\n",
    "print(vocabulary[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# We add an \"<UNK>\" token to represent all out-of-vocabulary words\n",
    "vocabulary = [\"<UNK>\"] + vocabulary\n",
    "\n",
    "# Now we invert the array to have a mapping of words to indices\n",
    "word2index = {word:i for i,word in enumerate(vocabulary)}\n",
    "\n",
    "print(vocabulary[10])\n",
    "print(word2index[\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Mind your own business, you troll.\n",
      "\n",
      "Tokenized sentence: ['mind', 'your', 'own', 'business', ',', 'you', 'troll', '.']\n",
      "\n",
      "Vectorized: [ 1.  1.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Let's create a count vectorization of every sentence. \n",
    "# The value at vector[i] will be the number of times vocabulary[i] appears in the sentence.\n",
    "def count_vectorize(sent):\n",
    "    vect = np.zeros(len(vocabulary))\n",
    "    for word in sent:\n",
    "        vect[word2index.get(word, 0)] += 1\n",
    "    \n",
    "    return vect\n",
    "\n",
    "print(\"Original sentence:\", sentences[200])\n",
    "print(\"\")\n",
    "print(\"Tokenized sentence:\", tokenized[200])\n",
    "print(\"\")\n",
    "print(\"Vectorized:\", count_vectorize(tokenized[200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize all of the training data\n",
    "features = np.stack([count_vectorize(sent) for sent in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1001)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Now we have both the features and the labels\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9500, 1001)\n",
      "(500, 1001)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 95%/5%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.05)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Hmmm would I really make make the starter file have the best model...\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.48      0.64       446\n",
      "          1       0.18      0.94      0.30        54\n",
      "\n",
      "avg / total       0.90      0.53      0.60       500\n",
      "\n",
      "0.526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(sum(y_test == y_pred)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 1\n",
      " 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0\n",
      " 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0\n",
      " 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1\n",
      " 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0\n",
      " 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0\n",
      " 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
      " 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0\n",
      " 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0\n",
      " 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
      " 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
      " 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1\n",
      " 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0\n",
      " 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0\n",
      " 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1\n",
      " 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1\n",
      " 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's train on all of the data \n",
    "clf.fit(features, labels)\n",
    "\n",
    "# Prepare the testing data\n",
    "test_tokenized = [word_tokenize(sent.lower()) for sent in test_sentences]\n",
    "\n",
    "# Count vectorize the sentences\n",
    "test_features = np.stack([count_vectorize(sent) for sent in test_tokenized])\n",
    "\n",
    "y_pred = clf.predict(test_features)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the results to a file\n",
    "open(\"predictions.csv\", \"w+\").writelines([str(pred) + \"\\n\" for pred in y_pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
